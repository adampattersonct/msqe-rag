{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RAG Chatbot with Local HuggingFace Model\n\nA complete Retrieval-Augmented Generation (RAG) system for students - **NO API TOKEN NEEDED!**\n\n## What This Notebook Does\n\n1. **Loads PDF documents** from the `documents/` folder\n2. **Chunks text** into smaller pieces with overlap\n3. **Creates embeddings** (vector representations) locally\n4. **Builds a search index** using FAISS\n5. **Retrieves relevant chunks** for your questions\n6. **Generates answers** using a local HuggingFace model (runs on your computer)\n\n## Requirements\n\n- **NO API token needed** - everything runs locally!\n- PDF files in the `documents/` folder\n- Required packages: `pypdf`, `sentence-transformers`, `faiss-cpu`, `transformers`, `gradio`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Configuration (Global Variables)\n\n**IMPORTANT**: This cell configures the local model - no API token needed!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# GLOBAL CONFIGURATION\n# ============================================\n\n# Local Model Configuration (No API token needed!)\nMODEL_NAME = \"google/flan-t5-large\"  # You can also use \"google/flan-t5-base\" for faster, smaller model\n\n# Folder containing your PDF documents\nPDF_FOLDER = \"documents\"\n\nprint(\"‚úÖ Configuration loaded!\")\nprint(f\"   Local Model: {MODEL_NAME}\")\nprint(f\"   PDF Folder: {PDF_FOLDER}\")\nprint(f\"   üéâ No API token needed - everything runs locally!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom typing import List, Dict\nfrom pathlib import Path\n\n# PDF processing\nfrom pypdf import PdfReader\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\n\n# Vector store\nimport faiss\nimport numpy as np\n\n# Local LLM\nfrom transformers import pipeline\n\n# UI (optional)\nimport gradio as gr\n\nprint(\"‚úÖ All libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Initialize the RAG System\n\nThis sets up:\n- Local HuggingFace model (downloads once, then cached)\n- Local embedding model (runs on your computer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"üöÄ INITIALIZING RAG CHATBOT\")\nprint(\"=\" * 60)\n\n# Load embedding model (this runs locally)\nprint(\"\\n‚úì Loading embedding model (local)...\")\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nprint(\"  ‚úì Embedding model loaded\")\n\n# Load local LLM for answer generation\nprint(f\"\\n‚úì Loading language model: {MODEL_NAME}\")\nprint(\"  (This may take a minute on first run - model will be downloaded and cached)\")\nllm = pipeline(\n    \"text2text-generation\",\n    model=MODEL_NAME,\n    max_length=512,\n    device=-1  # Use CPU (-1) or GPU (0) if available\n)\nprint(\"  ‚úì Language model loaded and ready!\")\n\n# Initialize storage\nchunks = []\nchunk_embeddings = None\nindex = None\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ RAG CHATBOT READY - 100% LOCAL!\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load PDF Documents\n",
    "\n",
    "This function extracts text from all PDFs in your `documents/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined: load_pdfs()\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs(pdf_folder: str = PDF_FOLDER) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load and extract text from all PDFs in folder\n",
    "    \n",
    "    Args:\n",
    "        pdf_folder: Path to folder containing PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of text strings, one per PDF\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Loading PDFs from '{pdf_folder}/'...\")\n",
    "\n",
    "    all_text = []\n",
    "    pdf_folder_path = Path(pdf_folder)\n",
    "    pdf_folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_folder_path.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"  ‚ö†Ô∏è  No PDF files found in '{pdf_folder}/'\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  Found {len(pdf_files)} PDF file(s)\")\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"  Processing: {pdf_path.name}\")\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            all_text.append(text)\n",
    "            print(f\"    ‚úì Extracted {len(reader.pages)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Error: {e}\")\n",
    "\n",
    "    return all_text\n",
    "\n",
    "print(\"‚úÖ Function defined: load_pdfs()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Chunk Text with Overlap\n",
    "\n",
    "Breaking documents into chunks helps with:\n",
    "- More precise retrieval\n",
    "- Staying within model limits\n",
    "- Better matching with queries\n",
    "\n",
    "**Overlap** ensures we don't lose context at chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined: chunk_text()\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(texts: List[str], chunk_size: int = 500, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split texts into smaller chunks with overlap\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to chunk\n",
    "        chunk_size: Target size of each chunk in characters\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚úÇÔ∏è  Chunking text (size={chunk_size}, overlap={overlap})...\")\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for text in texts:\n",
    "        sentences = text.replace('\\n', ' ').split('. ')\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            sentence_length = len(sentence) + 2\n",
    "\n",
    "            if current_length + sentence_length > chunk_size and current_chunk:\n",
    "                chunk_text = '. '.join(current_chunk) + '.'\n",
    "                chunks.append(chunk_text)\n",
    "\n",
    "                if overlap > 0:\n",
    "                    overlap_text = chunk_text[-overlap:]\n",
    "                    current_chunk = [overlap_text + sentence]\n",
    "                    current_length = len('. '.join(current_chunk))\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunk_text = '. '.join(current_chunk)\n",
    "            if not chunk_text.endswith('.'):\n",
    "                chunk_text += '.'\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "    print(f\"  ‚úì Created {len(chunks)} chunks\")\n",
    "    if chunks:\n",
    "        print(f\"  ‚úì Avg length: {sum(len(c) for c in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "print(\"‚úÖ Function defined: chunk_text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Vector Store\n",
    "\n",
    "This creates:\n",
    "1. **Embeddings**: Vector representations of text chunks\n",
    "2. **FAISS Index**: Fast similarity search structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined: create_vector_store()\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(text_chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Create embeddings and FAISS index\n",
    "    \n",
    "    Args:\n",
    "        text_chunks: List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (embeddings, index)\n",
    "    \"\"\"\n",
    "    global chunk_embeddings, index\n",
    "    \n",
    "    print(f\"\\nüî¢ Creating vector store...\")\n",
    "\n",
    "    print(f\"  Encoding {len(text_chunks)} chunks...\")\n",
    "    chunk_embeddings = embedder.encode(\n",
    "        text_chunks,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    print(f\"  Building FAISS index...\")\n",
    "    dimension = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(chunk_embeddings.astype('float32'))\n",
    "\n",
    "    print(f\"  ‚úì Vector store ready ({index.ntotal} vectors)\")\n",
    "    \n",
    "    return chunk_embeddings, index\n",
    "\n",
    "print(\"‚úÖ Function defined: create_vector_store()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setup Knowledge Base\n",
    "\n",
    "**Run this cell to process your PDFs!**\n",
    "\n",
    "This will:\n",
    "1. Load all PDFs from `documents/` folder\n",
    "2. Chunk the text\n",
    "3. Create embeddings and search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Loading PDFs from 'documents/'...\n",
      "  Found 1 PDF file(s)\n",
      "  Processing: intro-to-econometrics.pdf\n",
      "    ‚úì Extracted 801 pages\n",
      "\n",
      "‚úÇÔ∏è  Chunking text (size=500, overlap=100)...\n",
      "  ‚úì Created 6431 chunks\n",
      "  ‚úì Avg length: 434 chars\n",
      "\n",
      "üî¢ Creating vector store...\n",
      "  Encoding 6431 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [03:53<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building FAISS index...\n",
      "  ‚úì Vector store ready (6431 vectors)\n",
      "\n",
      "============================================================\n",
      "‚úÖ KNOWLEDGE BASE READY\n",
      "   Total chunks: 6431\n",
      "   Embedding dim: 384\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load PDFs\n",
    "documents = load_pdfs(PDF_FOLDER)\n",
    "\n",
    "if not documents:\n",
    "    print(\"\\n‚ùå No PDFs loaded. Please add PDFs to the 'documents/' folder.\")\n",
    "else:\n",
    "    # Chunk texts\n",
    "    chunks = chunk_text(documents)\n",
    "\n",
    "    # Create vector store\n",
    "    chunk_embeddings, index = create_vector_store(chunks)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ KNOWLEDGE BASE READY\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Embedding dim: {chunk_embeddings.shape[1]}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Retrieval Function\n",
    "\n",
    "This finds the most relevant chunks for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined: retrieve()\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query: str, top_k: int = 3) -> tuple:\n",
    "    \"\"\"\n",
    "    Retrieve most relevant chunks for a query\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        top_k: Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (relevant_chunks, similarities)\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode([query])\n",
    "\n",
    "    # Search\n",
    "    distances, indices = index.search(\n",
    "        query_embedding.astype('float32'),\n",
    "        min(top_k, len(chunks))\n",
    "    )\n",
    "\n",
    "    # Convert distances to similarities\n",
    "    similarities = 1 / (1 + distances[0])\n",
    "\n",
    "    # Get chunks\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "    return relevant_chunks, similarities\n",
    "\n",
    "print(\"‚úÖ Function defined: retrieve()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Answer Generation with Local Model\n\nThis uses the local HuggingFace model to generate answers based on retrieved context.\n\n**Note**: Uses the global `llm` pipeline configured in Step 3."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_answer(query: str, context_chunks: List[str]) -> str:\n    \"\"\"\n    Generate answer using local LLM\n    \n    Args:\n        query: User's question\n        context_chunks: Relevant context chunks\n        \n    Returns:\n        Generated answer\n    \"\"\"\n    # Combine context (keep it reasonable for the model)\n    context_text = \"\\n\".join(context_chunks)\n    \n    # Truncate if too long (FLAN-T5 has token limits)\n    max_context_length = 1000\n    if len(context_text) > max_context_length:\n        context_text = context_text[:max_context_length] + \"...\"\n    \n    # Create prompt optimized for FLAN-T5\n    prompt = f\"\"\"Answer the question based only on the context below.\n\nContext: {context_text}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n    \n    try:\n        # Generate answer using local model\n        response = llm(prompt, max_length=150, do_sample=False)[0]['generated_text']\n        return response.strip()\n    except Exception as e:\n        return f\"Error generating answer: {str(e)}\"\n\nprint(\"‚úÖ Function defined: generate_answer()\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Complete RAG Pipeline\n",
    "\n",
    "This combines retrieval + generation into one simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function defined: ask()\n",
      "\n",
      "üéâ RAG pipeline ready! Try asking questions in the next cell.\n"
     ]
    }
   ],
   "source": [
    "def ask(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve + generate\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with answer, sources, and similarities\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        return {\n",
    "            \"answer\": \"‚ùå Knowledge base not set up.\",\n",
    "            \"sources\": [],\n",
    "            \"similarities\": []\n",
    "        }\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    relevant_chunks, similarities = retrieve(question, top_k)\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(question, relevant_chunks)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": relevant_chunks,\n",
    "        \"similarities\": similarities\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Function defined: ask()\")\n",
    "print(\"\\nüéâ RAG pipeline ready! Try asking questions in the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test the RAG System!\n",
    "\n",
    "Ask questions about your documents here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "‚ùì Q: What is regression analysis?\n",
      "üí¨ A: ‚ùå Invalid HuggingFace token. Please check your token.\n",
      "üìä Similarities: ['0.54', '0.54', '0.53']\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: What is econometrics?\n",
      "üí¨ A: ‚ùå Invalid HuggingFace token. Please check your token.\n",
      "üìä Similarities: ['0.76', '0.70', '0.65']\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ùì Q: What are the main topics in this book?\n",
      "üí¨ A: ‚ùå Invalid HuggingFace token. Please check your token.\n",
      "üìä Similarities: ['0.51', '0.51', '0.50']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is regression analysis?\",\n",
    "    \"What is econometrics?\",\n",
    "    \"What are the main topics in this book?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING RAG SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n‚ùì Q: {q}\")\n",
    "    result = ask(q, top_k=3)\n",
    "    print(f\"üí¨ A: {result['answer']}\")\n",
    "    print(f\"üìä Similarities: {[f'{s:.2f}' for s in result['similarities']]}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Interactive Q&A\n",
    "\n",
    "Ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Your Question: What is regression analysis?\n",
      "\n",
      "üí¨ Answer:\n",
      "‚ùå Invalid HuggingFace token. Please check your token.\n",
      "\n",
      "üìö Source Chunks:\n",
      "\n",
      "1. (Similarity: 0.54)\n",
      "   ceptual framework used in this text is the multiple regression model, the  mainstay of econometrics.This model, introduced in Part II, provides a mathematical way to quantify how a change in one varia...\n",
      "\n",
      "2. (Similarity: 0.54)\n",
      "   OLS algorithm. Regression software typically computes  the¬†OLS fixed effects estimator in two steps.In the first step, the entity-specific average is subtracted from each variable. In the second step,...\n",
      "\n",
      "3. (Similarity: 0.53)\n",
      "   s explain how to use multiple regression to analyze the  relationship among variables in a data set.In this chapter, we step back and ask,  What makes a study that uses multiple regression reliable or...\n"
     ]
    }
   ],
   "source": [
    "# Ask your own question here\n",
    "my_question = \"What is regression analysis?\"  # Change this!\n",
    "\n",
    "print(f\"\\n‚ùì Your Question: {my_question}\\n\")\n",
    "\n",
    "result = ask(my_question, top_k=3)\n",
    "\n",
    "print(f\"üí¨ Answer:\\n{result['answer']}\\n\")\n",
    "\n",
    "print(\"üìö Source Chunks:\")\n",
    "for i, (source, sim) in enumerate(zip(result['sources'], result['similarities']), 1):\n",
    "    print(f\"\\n{i}. (Similarity: {sim:.2f})\")\n",
    "    print(f\"   {source[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Launch Gradio Interface (Optional)\n",
    "\n",
    "Create a web interface for easier interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_gradio_interface():\n    \"\"\"\n    Create Gradio web interface\n    \"\"\"\n    def chatbot_response(message, history):\n        result = ask(message, top_k=3)\n\n        response = f\"{result['answer']}\\n\\n\"\n\n        if result['sources']:\n            response += \"---\\n**üìö Sources:**\\n\"\n            for i, (source, sim) in enumerate(\n                zip(result['sources'][:2], result['similarities'][:2]),\n                1\n            ):\n                source_preview = source[:100].replace('\\n', ' ')\n                response += f\"\\n{i}. (Similarity: {sim:.2f}) {source_preview}...\\n\"\n\n        return response\n\n    interface = gr.ChatInterface(\n        chatbot_response,\n        title=\"üìö RAG Chatbot - 100% Local!\",\n        description=f\"Ask questions about your PDF documents! Uses {MODEL_NAME} running locally - no API needed!\",\n        examples=[\n            \"What is this document about?\",\n            \"Explain the main concept\",\n            \"Summarize the key points\",\n        ],\n        theme=\"soft\",\n    )\n\n    return interface\n\n# Launch the interface\nprint(\"üåê Launching Gradio interface...\")\nprint(\"   Press the stop button in Jupyter to stop the server.\\n\")\n\ndemo = create_gradio_interface()\ndemo.launch(share=False)  # Set share=True for a public link"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've built a complete RAG system! Here's what you learned:\n\n### Components\n1. **PDF Processing** - Extract text from documents\n2. **Text Chunking** - Split into manageable pieces with overlap\n3. **Embeddings** - Create vector representations (local)\n4. **FAISS Index** - Fast similarity search\n5. **Retrieval** - Find relevant chunks\n6. **Local LLM** - Generate answers using HuggingFace transformers\n\n### Key Features\n- ‚úÖ **100% Free** - No API tokens or costs\n- ‚úÖ **100% Local** - Everything runs on your computer\n- ‚úÖ **Privacy** - Your documents never leave your machine\n- ‚úÖ **Production-ready** - Same patterns used in real applications\n- ‚úÖ **Works with HuggingFace Hub** - Uses transformers library\n\n### Models Used\n- **Embeddings**: `all-MiniLM-L6-v2` (sentence-transformers)\n- **Generation**: `google/flan-t5-large` (can change to `flan-t5-base` for speed)\n\n### Next Steps\n- Try different questions\n- Add more PDF documents\n- Experiment with `top_k` parameter\n- Try the Gradio interface\n- Switch to `google/flan-t5-base` for faster responses (change in Step 1)\n\n### Troubleshooting\n- **Slow responses**: Try using `google/flan-t5-base` instead of `flan-t5-large`\n- **Out of memory**: Reduce chunk size or use smaller model\n- **No PDFs found**: Add PDFs to `documents/` folder\n- **Model download issues**: Check internet connection - models download once and are cached"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msqe-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}